# Load demographic data
zip_demographics <- ZipDemography  # From DataComputing package
zip_geography <- ZipGeography      # From DataComputing package
# Join with accident data
enriched_data <- accident_data %>%
left_join(zip_demographics, by = c("Zipcode" = "Zip")) %>%
left_join(zip_geography, by = c("Zipcode" = "Zip"))
# Analysis by income levels
income_analysis <- enriched_data %>%
mutate(income_bracket = cut(MedianIncome,
breaks = quantile(MedianIncome, probs = seq(0, 1, 0.25), na.rm = TRUE),
labels = c("Low", "Medium-Low", "Medium-High", "High"))) %>%
group_by(income_bracket) %>%
summarise(
accident_count = n(),
avg_severity = mean(Severity),
population_density = mean(Population / LandArea)
)
return(income_analysis)
}
# 4. Road Infrastructure Analysis
infrastructure_analysis <- function(accident_data) {
# Create binary features for infrastructure
infrastructure_impact <- accident_data %>%
mutate(across(c(Junction, Crossing, Traffic_Signal,
Railway, Roundabout), as.logical)) %>%
group_by(Severity) %>%
summarise(
pct_with_signals = mean(Traffic_Signal) * 100,
pct_at_junction = mean(Junction) * 100,
pct_at_crossing = mean(Crossing) * 100,
pct_near_railway = mean(Railway) * 100,
pct_at_roundabout = mean(Roundabout) * 100
)
return(infrastructure_impact)
}
# Required packages
library(tidyverse)
library(lubridate)
library(DataComputing)  # For ZipGeography and ZipDemography
# 1. Temporal Analysis
temporal_analysis <- function(accident_data) {
# Convert timestamps to proper datetime
accident_data <- accident_data %>%
mutate(
Start_Time = ymd_hms(Start_Time),
hour = hour(Start_Time),
month = month(Start_Time),
day_of_week = wday(Start_Time, label = TRUE),
is_weekend = if_else(day_of_week %in% c("Sat", "Sun"), "Weekend", "Weekday")
)
# Hourly patterns
hourly_patterns <- accident_data %>%
group_by(hour, is_weekend) %>%
summarise(accident_count = n()) %>%
ggplot(aes(x = hour, y = accident_count, color = is_weekend)) +
geom_line() +
labs(title = "Hourly Accident Patterns", x = "Hour of Day", y = "Number of Accidents")
return(hourly_patterns)
}
# 2. Weather Impact Analysis
weather_analysis <- function(accident_data) {
weather_summary <- accident_data %>%
group_by(Weather_Condition) %>%
summarise(
avg_severity = mean(Severity),
accident_count = n(),
avg_distance = mean(`Distance(mi)`),
avg_temperature = mean(`Temperature(F)`)
) %>%
arrange(desc(accident_count))
# Correlation between weather metrics and severity
weather_correlation <- accident_data %>%
select(Severity, `Temperature(F)`, `Wind_Speed(mph)`,
`Visibility(mi)`, `Precipitation(in)`) %>%
cor()
return(list(summary = weather_summary, correlation = weather_correlation))
}
# 3. Spatial Analysis with Demographics
spatial_demographic_analysis <- function(accident_data) {
# Load demographic data
zip_demographics <- ZipDemography  # From DataComputing package
zip_geography <- ZipGeography      # From DataComputing package
# Join with accident data
enriched_data <- accident_data %>%
left_join(zip_demographics, by = c("Zipcode" = "Zip")) %>%
left_join(zip_geography, by = c("Zipcode" = "Zip"))
# Analysis by income levels
income_analysis <- enriched_data %>%
mutate(income_bracket = cut(MedianIncome,
breaks = quantile(MedianIncome, probs = seq(0, 1, 0.25), na.rm = TRUE),
labels = c("Low", "Medium-Low", "Medium-High", "High"))) %>%
group_by(income_bracket) %>%
summarise(
accident_count = n(),
avg_severity = mean(Severity),
population_density = mean(Population / LandArea)
)
return(income_analysis)
}
# 4. Road Infrastructure Analysis
infrastructure_analysis <- function(accident_data) {
# Create binary features for infrastructure
infrastructure_impact <- accident_data %>%
mutate(across(c(Junction, Crossing, Traffic_Signal,
Railway, Roundabout), as.logical)) %>%
group_by(Severity) %>%
summarise(
pct_with_signals = mean(Traffic_Signal) * 100,
pct_at_junction = mean(Junction) * 100,
pct_at_crossing = mean(Crossing) * 100,
pct_near_railway = mean(Railway) * 100,
pct_at_roundabout = mean(Roundabout) * 100
)
return(infrastructure_impact)
}
# Required packages
library(tidyverse)
library(lubridate)
library(DataComputing)  # For ZipGeography and ZipDemography
# 1. Temporal Analysis
# Convert timestamps to proper datetime
accident_data <- accident_data %>%
mutate(
Start_Time = ymd_hms(Start_Time),
hour = hour(Start_Time),
month = month(Start_Time),
day_of_week = wday(Start_Time, label = TRUE),
is_weekend = if_else(day_of_week %in% c("Sat", "Sun"), "Weekend", "Weekday")
)
# Hourly patterns
hourly_patterns <- accident_data %>%
group_by(hour, is_weekend) %>%
summarise(accident_count = n()) %>%
ggplot(aes(x = hour, y = accident_count, color = is_weekend)) +
geom_line() +
labs(title = "Hourly Accident Patterns", x = "Hour of Day", y = "Number of Accidents")
# 2. Weather Impact Analysis
weather_analysis <- function(accident_data) {
weather_summary <- accident_data %>%
group_by(Weather_Condition) %>%
summarise(
avg_severity = mean(Severity),
accident_count = n(),
avg_distance = mean(`Distance(mi)`),
avg_temperature = mean(`Temperature(F)`)
) %>%
arrange(desc(accident_count))
# Correlation between weather metrics and severity
weather_correlation <- accident_data %>%
select(Severity, `Temperature(F)`, `Wind_Speed(mph)`,
`Visibility(mi)`, `Precipitation(in)`) %>%
cor()
return(list(summary = weather_summary, correlation = weather_correlation))
}
# 3. Spatial Analysis with Demographics
spatial_demographic_analysis <- function(accident_data) {
# Load demographic data
zip_demographics <- ZipDemography  # From DataComputing package
zip_geography <- ZipGeography      # From DataComputing package
# Join with accident data
enriched_data <- accident_data %>%
left_join(zip_demographics, by = c("Zipcode" = "Zip")) %>%
left_join(zip_geography, by = c("Zipcode" = "Zip"))
# Analysis by income levels
income_analysis <- enriched_data %>%
mutate(income_bracket = cut(MedianIncome,
breaks = quantile(MedianIncome, probs = seq(0, 1, 0.25), na.rm = TRUE),
labels = c("Low", "Medium-Low", "Medium-High", "High"))) %>%
group_by(income_bracket) %>%
summarise(
accident_count = n(),
avg_severity = mean(Severity),
population_density = mean(Population / LandArea)
)
return(income_analysis)
}
# 4. Road Infrastructure Analysis
infrastructure_analysis <- function(accident_data) {
# Create binary features for infrastructure
infrastructure_impact <- accident_data %>%
mutate(across(c(Junction, Crossing, Traffic_Signal,
Railway, Roundabout), as.logical)) %>%
group_by(Severity) %>%
summarise(
pct_with_signals = mean(Traffic_Signal) * 100,
pct_at_junction = mean(Junction) * 100,
pct_at_crossing = mean(Crossing) * 100,
pct_near_railway = mean(Railway) * 100,
pct_at_roundabout = mean(Roundabout) * 100
)
return(infrastructure_impact)
}
View(accident_data)
# Required packages
library(tidyverse)
library(lubridate)
library(DataComputing)  # For ZipGeography and ZipDemography
# 1. Temporal Analysis
# Convert timestamps to proper datetime
accident_data <- accident_data %>%
mutate(
Start_Time = ymd_hms(Start_Time),
hour = hour(Start_Time),
month = month(Start_Time),
day_of_week = wday(Start_Time, label = TRUE),
is_weekend = if_else(day_of_week %in% c("Sat", "Sun"), "Weekend", "Weekday")
)
# Hourly patterns
hourly_patterns <- accident_data %>%
group_by(hour, is_weekend) %>%
summarise(accident_count = n()) %>%
ggplot(aes(x = hour, y = accident_count, color = is_weekend)) +
geom_line() +
labs(title = "Hourly Accident Patterns", x = "Hour of Day", y = "Number of Accidents")
# 2. Weather Impact Analysis
weather_summary <- accident_data %>%
group_by(Weather_Condition) %>%
summarise(
avg_severity = mean(Severity),
accident_count = n(),
avg_distance = mean(`Distance(mi)`),
avg_temperature = mean(`Temperature(F)`)
) %>%
arrange(desc(accident_count))
# Correlation between weather metrics and severity
weather_correlation <- accident_data %>%
select(Severity, `Temperature(F)`, `Wind_Speed(mph)`,
`Visibility(mi)`, `Precipitation(in)`) %>%
cor()
# 3. Spatial Analysis with Demographics
spatial_demographic_analysis <- function(accident_data) {
# Load demographic data
zip_demographics <- ZipDemography  # From DataComputing package
zip_geography <- ZipGeography      # From DataComputing package
# Join with accident data
enriched_data <- accident_data %>%
left_join(zip_demographics, by = c("Zipcode" = "Zip")) %>%
left_join(zip_geography, by = c("Zipcode" = "Zip"))
# Analysis by income levels
income_analysis <- enriched_data %>%
mutate(income_bracket = cut(MedianIncome,
breaks = quantile(MedianIncome, probs = seq(0, 1, 0.25), na.rm = TRUE),
labels = c("Low", "Medium-Low", "Medium-High", "High"))) %>%
group_by(income_bracket) %>%
summarise(
accident_count = n(),
avg_severity = mean(Severity),
population_density = mean(Population / LandArea)
)
return(income_analysis)
}
# 4. Road Infrastructure Analysis
infrastructure_analysis <- function(accident_data) {
# Create binary features for infrastructure
infrastructure_impact <- accident_data %>%
mutate(across(c(Junction, Crossing, Traffic_Signal,
Railway, Roundabout), as.logical)) %>%
group_by(Severity) %>%
summarise(
pct_with_signals = mean(Traffic_Signal) * 100,
pct_at_junction = mean(Junction) * 100,
pct_at_crossing = mean(Crossing) * 100,
pct_near_railway = mean(Railway) * 100,
pct_at_roundabout = mean(Roundabout) * 100
)
return(infrastructure_impact)
}
library(esquisse)
esquisser(weather_correlation)
View(weather_correlation)
library(readr)
library(dplyr)
#Using data.table library to use 'fread'. I found this resource while researching online and is used to load large datasets. This is much faster than read_csv
library(data.table)
library(tidyverse)
accident_data <- fread("US_Accidents_March23_SMALL.csv")
head(accident_data, 4)
cleaned_accident_data <- accident_data %>%
filter(!is.na("Severity"))
clear
cleaned_accident_data <- accident_data %>%
filter(!is.na("Zipcode"))
cleaned_accident_data <- accident_data %>%
filter(!is.na("Zipcode"))
cleaned_accident_data <- accident_data %>%
filter(
!is.na(Severity),
!is.na(Start_Time),
!is.na(Start_Lat),
!is.na(Start_Lng),
!is.na(Zipcode),
!is.na(Weather_Condition),
!is.na(`Temperature(F)`),
!is.na(`Visibility(mi)`)
)
View(cleaned_accident_data)
cleaned_accident_data <- accident_data %>%
filter(
!is.na(Severity),
!is.na(Start_Time),
!is.na(Zipcode),
!is.na(Weather_Condition),
!is.na(`Temperature(F)`),
!is.na(`Visibility(mi)`),
!is.na(`Precipitation(in)`)
)
weather_summary <- cleaned_accident_data %>%
group_by(Weather_Condition) %>%
summarise(
count = n(),
avg_severity = mean(Severity),
avg_temp = mean(`Temperature(F)`),
avg_visibility = mean(`Visibility(mi)`),
avg_precipitation = mean(`Precipitation(in)`)
) %>%
arrange(desc(count))
View(weather_summary)
library(readr)
library(dplyr)
#Using data.table library to use 'fread'. I found this resource while researching online and is used to load large datasets. This is much faster than read_csv
library(data.table)
library(tidyverse)
accident_data <- fread("US_Accidents_March23.csv")
head(accident_data, 4)
cleaned_accident_data <- accident_data %>%
filter(
!is.na(Severity),
!is.na(Start_Time),
!is.na(Zipcode),
!is.na(Weather_Condition),
!is.na(`Temperature(F)`),
!is.na(`Visibility(mi)`),
!is.na(`Precipitation(in)`)
)
weather_summary <- cleaned_accident_data %>%
group_by(Weather_Condition) %>%
summarise(
count = n(),
avg_severity = mean(Severity),
avg_temp = mean(`Temperature(F)`),
avg_visibility = mean(`Visibility(mi)`),
avg_precipitation = mean(`Precipitation(in)`)
) %>%
arrange(desc(count))
View(weather_summary)
library(readr)
library(dplyr)
#Using data.table library to use 'fread'. I found this resource while researching online and is used to load large datasets. This is much faster than read_csv
library(data.table)
library(tidyverse)
accident_data <- fread("US_Accidents_March23_SMALL.csv")
head(accident_data, 4)
cleaned_accident_data <- accident_data %>%
filter(
!is.na(Severity),
!is.na(Start_Time),
!is.na(Zipcode),
!is.na(Weather_Condition),
!is.na(`Temperature(F)`),
!is.na(`Visibility(mi)`),
!is.na(`Precipitation(in)`)
)
weather_summary <- cleaned_accident_data %>%
group_by(Weather_Condition) %>%
summarise(
count = n(),
avg_severity = mean(Severity),
avg_temp = mean(`Temperature(F)`),
avg_visibility = mean(`Visibility(mi)`),
avg_precipitation = mean(`Precipitation(in)`)
) %>%
arrange(desc(count))
ggplot(weather_summary,
aes(x = reorder(Weather_Condition, count), y = count)) +
geom_bar(stat = "identity", aes(fill = avg_severity)) +
coord_flip() +
scale_fill_viridis_c() +
theme_minimal() +
labs(
title = "Accident Frequency and Severity by Weather Condition",
x = "Weather Condition",
y = "Number of Accidents",
fill = "Average\nSeverity"
)
cleaned_accident_data <- cleaned_accident_data %>%
mutate(
Temp_Category = case_when(
`Temperature(F)` < 32 ~ "Below Freezing",
`Temperature(F)` >= 32 & `Temperature(F)` < 50 ~ "Cold",
`Temperature(F)` >= 50 & `Temperature(F)` < 70 ~ "Mild",
`Temperature(F)` >= 70 & `Temperature(F)` < 90 ~ "Warm",
`Temperature(F)` >= 90 ~ "Hot"
)
)
cleaned_accident_data <- cleaned_accident_data %>%
mutate(
Temp_Category = case_when(
`Temperature(F)` < 32 ~ "Below Freezing",
`Temperature(F)` >= 32 & `Temperature(F)` < 50 ~ "Cold",
`Temperature(F)` >= 50 & `Temperature(F)` < 70 ~ "Mild",
`Temperature(F)` >= 70 & `Temperature(F)` < 90 ~ "Warm",
`Temperature(F)` >= 90 ~ "Hot"
)
)
ggplot(cleaned_accident_data, aes(x = Temp_Category)) +
geom_bar(fill = "skyblue") +
labs(title = "Accident Frequency by Temperature Category",
x = "Temperature Category",
y = "Number of Accidents") +
theme_minimal()
cleaned_accident_data <- cleaned_accident_data %>%
mutate(
Temp_Category = case_when(
`Temperature(F)` < 32 ~ "Below Freezing",
`Temperature(F)` >= 32 & `Temperature(F)` < 50 ~ "Cold",
`Temperature(F)` >= 50 & `Temperature(F)` < 70 ~ "Mild",
`Temperature(F)` >= 70 & `Temperature(F)` < 90 ~ "Warm",
`Temperature(F)` >= 90 ~ "Hot"
)
)
ggplot(cleaned_accident_data, aes(x = Temp_Category)) +
geom_bar(fill = "skyblue") +
labs(title = "Accident Frequency by Temperature Category",
x = "Temperature Category",
y = "Number of Accidents") +
theme_minimal()
ggplot(cleaned_accident_data, aes(x = Temp_Category, fill = Severity)) +
geom_bar(position = "fill") +
labs(title = "Severity Proportions by Temperature Category",
x = "Temperature Category",
y = "Proportion of Accidents") +
scale_fill_brewer(palette = "Reds") +
theme_minimal()
cleaned_accident_data <- cleaned_accident_data %>%
mutate(
Temp_Category = case_when(
`Temperature(F)` < 32 ~ "Below Freezing",
`Temperature(F)` >= 32 & `Temperature(F)` < 50 ~ "Cold",
`Temperature(F)` >= 50 & `Temperature(F)` < 70 ~ "Mild",
`Temperature(F)` >= 70 & `Temperature(F)` < 90 ~ "Warm",
`Temperature(F)` >= 90 ~ "Hot"
)
)
ggplot(weather_summary,
aes(x = reorder(Temp_Category, count), y = count)) +
geom_bar(stat = "identity", aes(fill = avg_severity)) +
coord_flip() +
scale_fill_viridis_c() +
theme_minimal() +
labs(
title = "Accident Frequency and Severity by Weather Condition",
x = "Weather Condition",
y = "Number of Accidents",
fill = "Average\nSeverity"
)
cleaned_accident_data <- cleaned_accident_data %>%
mutate(
Temperature_Group = case_when(
`Temperature(F)` < 32 ~ "Below Freezing",
`Temperature(F)` >= 32 & `Temperature(F)` < 50 ~ "Cold",
`Temperature(F)` >= 50 & `Temperature(F)` < 70 ~ "Mild",
`Temperature(F)` >= 70 & `Temperature(F)` < 90 ~ "Warm",
`Temperature(F)` >= 90 ~ "Hot"
)
)
temperature_summary <- cleaned_accident_data %>%
group_by(Temperature_Group) %>%
summarise(
count = n(),
avg_severity = mean(Severity),
) %>%
arrange(desc(count))
ggplot(temperature_summary,
aes(x = reorder(Temp_Category, count), y = count)) +
geom_bar(stat = "identity", aes(fill = avg_severity)) +
coord_flip() +
scale_fill_viridis_c() +
theme_minimal() +
labs(
title = "Accident Frequency and Severity by Weather Condition",
x = "Weather Condition",
y = "Number of Accidents",
fill = "Average\nSeverity"
)
View(temperature_summary)
library(readr)
library(dplyr)
#Using data.table library to use 'fread'. I found this resource while researching online and is used to load large datasets. This is much faster than read_csv
library(data.table)
library(tidyverse)
accident_data <- fread("US_Accidents_March23.csv")
head(accident_data, 4)
library(readr)
library(dplyr)
#Using data.table library to use 'fread'. I found this resource while researching online and is used to load large datasets. This is much faster than read_csv
library(data.table)
library(tidyverse)
library(dcData)
data("ZipDemography")
cleaned_accident_data <- fread("US_Accidents_March23.csv")
head(cleaned_accident_data, 4)
cleaned_cleaned_accident_data <- cleaned_accident_data %>%
filter(
!is.na(Severity),
!is.na(Start_Time),
!is.na(Zipcode),
!is.na(Weather_Condition),
!is.na(State)
) %>%
mutate(Start_Time = ymd_hms(Start_Time)) %>%
select(Severity, Start_Time, Zipcode, Weather_Condition, State)
